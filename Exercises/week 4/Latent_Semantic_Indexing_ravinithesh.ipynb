{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Indexing\n",
    "\n",
    "\n",
    "In this exercise, we will run latent semantic indexing on a term-document matrix using python numpy library.\n",
    "\n",
    "Suppose we are given the following term-document matrix containing eleven terms and four documents $d_1$ , $d_2$ , $d_3$ and $d_4$:\n",
    "\n",
    "$\n",
    "M =\n",
    "  \\begin{bmatrix}\n",
    "    d_1 & d_2 & d_3 & d_4 \\\\ \n",
    "\t1 & 1 & 1 & 1  \\\\\n",
    "\t0 & 1 & 1 & 1 \\\\\n",
    "\t1 & 0 & 0 & 0 \\\\\n",
    "\t0 & 1 & 0 & 0 \\\\\n",
    "    1 & 0 & 0 & 0 \\\\\n",
    "    1 & 0 & 1 & 2 \\\\\n",
    "    1 & 1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1 & 0 \\\\\n",
    "    1 & 0 & 0 & 0 \\\\\n",
    "    0 & 2 & 1 & 1 \\\\\n",
    "    0 & 1 & 1 & 0 \\\\\n",
    "  \\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "###  Question 1.a\n",
    "\n",
    "Compute the singular value decomposition of the term-document matrix M. Print the values of the output matrices $K$, $S$ and $D^t$.\n",
    "\n",
    "\n",
    "<b>Hint:</b> Use the function numpy.linalg.svd. More details of this function can be found here at this link:\n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html\n",
    "\n",
    "\n",
    "Here's sample code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Python matrix operations library\n",
    "import numpy as np\n",
    "\n",
    "#set M matrix using the given values.\n",
    "M = [[1,1,1,1], \n",
    "     [0,1,1,1],\n",
    "     [1,0,0,0],\n",
    "     [0,1,0,0],\n",
    "     [1,0,0,0],\n",
    "     [1,0,1,2],\n",
    "     [1,1,1,1],\n",
    "     [1,1,1,0],\n",
    "     [1,0,0,0],\n",
    "     [0,2,1,1],\n",
    "     [0,1,1,0]]\n",
    "\n",
    "\n",
    "M = np.array(M)\n",
    "\n",
    "# compute SVD\n",
    "# ... = np.linalg.svd(...)\n",
    "K, S, Dt = np.linalg.svd(M, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.41291701, -0.12294407,  0.05933248, -0.03660797],\n",
       "       [-0.3359611 ,  0.1962311 , -0.25246121,  0.11968319],\n",
       "       [-0.07695592, -0.31917516,  0.31179369, -0.15629115],\n",
       "       [-0.11909604,  0.2663899 ,  0.20432237, -0.52093504],\n",
       "       [-0.07695592, -0.31917516,  0.31179369, -0.15629115],\n",
       "       [-0.39922386, -0.49767812, -0.57172873,  0.04465203],\n",
       "       [-0.41291701, -0.12294407,  0.05933248, -0.03660797],\n",
       "       [-0.30751414, -0.01459992,  0.48607132,  0.40306708],\n",
       "       [-0.07695592, -0.31917516,  0.31179369, -0.15629115],\n",
       "       [-0.45505713,  0.462621  , -0.04813884, -0.40125186],\n",
       "       [-0.23055822,  0.30457524,  0.17427762,  0.55935823]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.78695453, 2.31848919, 1.762346  , 0.77705263])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.36838448, -0.57010731, -0.53356439, -0.50455879],\n",
       "       [-0.74000417,  0.61762211,  0.0885323 , -0.25119473],\n",
       "       [ 0.54948837,  0.36008671, -0.05294924, -0.75206148],\n",
       "       [-0.12144645, -0.40479395,  0.83944473, -0.34165065]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Question 1.b\n",
    "\n",
    "Are the values of $S$ sorted? Perform latent semantic indexing by selecting the first two largest singular values of the matrix $S$.\n",
    "\n",
    "<b>Hint:</b> See the lecture slides on latent semantic indexing for more details. A sub-matrix of a numpy matrix can be computed using indexing operations (see https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K_sel = K[...]\n",
    "# S_sel = ...\n",
    "# Dt_sel = Dt[...]\n",
    "\n",
    "K_sel = K[:,0:2]\n",
    "S_sel = np.diag(S)[0:2,0:2]\n",
    "Dt_sel = Dt[0:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 1.c\n",
    "\n",
    "Given the query $q$:\n",
    "\n",
    "$\n",
    "q =\n",
    "  \\begin{bmatrix}\n",
    "\t0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\\n",
    "  \\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "Map query $q$ into the new document space $D$. The new query is referred to as $q^*$. \n",
    "\n",
    "<b>Hint:</b> Use the formulation for mapping queries provided in the lecture slides. You can also use np.linalg.inv function for computing the inverse of a matrix.\n",
    "\n",
    "###  Question 1.d\n",
    "\n",
    "Arrange the documents based on the cosine similarity measure between $q^*$ and the new documents in the space $D$.\n",
    "\n",
    "<b>Hint:</b> Use the cosine similarity function from the previous exercise on vector space retrieval.\n",
    "\n",
    "###  Question 1.e\n",
    "\n",
    "Does the order of documents change if document $d_3$ is dropped? If yes, why? \n",
    "If no, how should $d_3$ be modified to change the document ordering?\n",
    "\n",
    "\n",
    "### Question 1.f [Optional]\n",
    "\n",
    "Run latent semantic indexing for the document collection presented in the previous exercise (presented here as well):\n",
    "\n",
    "  DocID | Document Text\n",
    "  ------|------------------\n",
    "  1     | How to Bake Breads Without Baking Recipes\n",
    "  2     | Smith Pies: Best Pies in London\n",
    "  3     | Numerical Recipes: The Art of Scientific Computing\n",
    "  4     | Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes\n",
    "  5     | Pastry: A Book of Best French Pastry Recipes\n",
    "\n",
    "Now, for the query $Q=$''<i>baking</i>'', find the top ranked documents according to LSI (use three singular values). \n",
    "\n",
    "<b>Hint:</b> Use the code for computing document_vectors from the last exercise. However note that document_vectors represent document-term matrix whereas LSI uses term-document matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.22662409,  0.11624731])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = np.array([0,0,0,0,0,1,0,0,0,1,1])\n",
    "\n",
    "q_star = np.dot(q, np.dot(K_sel, np.linalg.inv(S_sel)))\n",
    "\n",
    "q_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 1.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between the query and document d1 is -0.012057913278690475\n",
      "The similarity between the query and document d2 is 0.9388827727147444\n",
      "The similarity between the query and document d3 is 0.9524776244205609\n",
      "The similarity between the query and document d4 is 0.5931086268074786\n"
     ]
    }
   ],
   "source": [
    "# compute cosine similarity.\n",
    "    \n",
    "import math\n",
    "\n",
    "# Function for computing cosine similarity.\n",
    "def cosine_similarity(v1, v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy*1.0/math.sqrt(sumxx*sumyy)\n",
    "\n",
    "for col in range(Dt_sel.shape[1]):\n",
    "    sim = cosine_similarity(Dt_sel[:,col], q_star)\n",
    "    print(\"The similarity between the query and document d{} is {}\".format(col+1, sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 1.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between the query and new document d1 is 0.10125520472871019\n",
      "The similarity between the query and new document d2 is 0.9475215481378308\n",
      "The similarity between the query and new document d3 is 0.6873076021729541\n"
     ]
    }
   ],
   "source": [
    "Mn = [[1,1,1],\n",
    "      [0,1,1],\n",
    "      [1,0,0],\n",
    "      [0,1,0],\n",
    "      [1,0,0],\n",
    "      [1,0,2],\n",
    "      [1,1,1],\n",
    "      [1,1,0],\n",
    "      [1,0,0],\n",
    "      [0,2,1],\n",
    "      [0,1,0]]\n",
    "\n",
    "Mn = np.array(Mn)\n",
    "\n",
    "# compute SVD\n",
    "K, S, Dt = np.linalg.svd(Mn, full_matrices=False)\n",
    "\n",
    "# LSI select dimensions\n",
    "K_sel = K[:,0:2]\n",
    "S_sel = np.diag(S)[0:2,0:2]\n",
    "Dt_sel = Dt[0:2,:]\n",
    "\n",
    "# transform query and documents\n",
    "q = np.array([0,0,0,0,0,1,0,0,0,1,1])\n",
    "q_star =  np.dot( np.dot(q, K_sel), np.linalg.inv(S_sel))\n",
    "\n",
    "for col in range(Dt_sel.shape[1]):\n",
    "    sim = cosine_similarity(Dt_sel[:,col], q_star)\n",
    "    print(\"The similarity between the query and new document d{} is {}\".format(col+1, sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document ordering does not change even if d3 is dropped. \n",
    "\n",
    "As d3 has similar magnitude of d2 and d4, dropping d3 does not change the term and document space heavily.\n",
    "\n",
    "To change the ordering the direction of d3 should be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Question 1.f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ravin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"bread.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = set([item for sublist in documents for item in sublist])\n",
    "vocabulary = [word for word in vocabulary if word not in stopwords.words('english')]\n",
    "vocabulary.sort()\n",
    "\n",
    "# compute IDF, storing idf values in a dictionary\n",
    "def idf_values(vocabulary, documents):\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        idf[term] = math.log(num_documents/sum(term in document for document in documents), math.e)\n",
    "    return idf\n",
    "\n",
    "# Function to generate the vector for a document (with normalisation)\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] = idf[term] * counts[term]/max_count\n",
    "    return vector\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['art',\n",
       " 'bake',\n",
       " 'best',\n",
       " 'book',\n",
       " 'bread',\n",
       " 'cake',\n",
       " 'comput',\n",
       " 'french',\n",
       " 'london',\n",
       " 'numer',\n",
       " 'pastri',\n",
       " 'pie',\n",
       " 'quantiti',\n",
       " 'recip',\n",
       " 'scientif',\n",
       " 'smith',\n",
       " 'without']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between the query and new document d1 is 0.9980518678772611\n",
      "The similarity between the query and new document d2 is -0.6577609566355738\n",
      "The similarity between the query and new document d3 is -0.0023288750529669527\n",
      "The similarity between the query and new document d4 is 0.7231078789682566\n",
      "The similarity between the query and new document d5 is -0.6551062911362043\n"
     ]
    }
   ],
   "source": [
    "## take transpose of document vectors to convert to term document matrix.\n",
    "M = np.array(document_vectors).T\n",
    "\n",
    "\n",
    "## LSI.\n",
    "K, S, Dt = np.linalg.svd(M, full_matrices=False)\n",
    "K_sel = K[:,0:3]\n",
    "S_sel = np.diag(S)[0:3,0:3]\n",
    "Dt_sel = Dt[0:3,:]\n",
    "\n",
    "\n",
    "q = np.array([0]*len(vocabulary))\n",
    "\n",
    "#Set the term corresponding to baking = 1\n",
    "q[1] = 1\n",
    "q_star =  np.dot( np.dot(q, K_sel), np.linalg.inv(S_sel))\n",
    "\n",
    "\n",
    "for col in range(Dt_sel.shape[1]):\n",
    "    sim = cosine_similarity(Dt_sel[:,col], q_star)\n",
    "    print(\"The similarity between the query and new document d{} is {}\".format(col+1, sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
