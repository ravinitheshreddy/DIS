{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Indexing\n",
    "\n",
    "\n",
    "In this exercise, we will run latent semantic indexing on a term-document matrix using python numpy library.\n",
    "\n",
    "Suppose we are given the following term-document matrix containing eleven terms and four documents $d_1$ , $d_2$ , $d_3$ and $d_4$:\n",
    "\n",
    "$\n",
    "M =\n",
    "  \\begin{bmatrix}\n",
    "    d_1 & d_2 & d_3 & d_4 \\\\ \n",
    "\t1 & 1 & 1 & 1  \\\\\n",
    "\t0 & 1 & 1 & 1 \\\\\n",
    "\t1 & 0 & 0 & 0 \\\\\n",
    "\t0 & 1 & 0 & 0 \\\\\n",
    "    1 & 0 & 0 & 0 \\\\\n",
    "    1 & 0 & 1 & 2 \\\\\n",
    "    1 & 1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1 & 0 \\\\\n",
    "    1 & 0 & 0 & 0 \\\\\n",
    "    0 & 2 & 1 & 1 \\\\\n",
    "    0 & 1 & 1 & 0 \\\\\n",
    "  \\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "###  Question 1.a\n",
    "\n",
    "Compute the singular value decomposition of the term-document matrix M. Print the values of the output matrices $K$, $S$ and $D^t$.\n",
    "\n",
    "\n",
    "<b>Hint:</b> Use the function numpy.linalg.svd. More details of this function can be found here at this link:\n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html\n",
    "\n",
    "\n",
    "Here's sample code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Python matrix operations library\n",
    "import numpy as np\n",
    "\n",
    "#set M matrix using the given values.\n",
    "M = [[1,1,1,1], \n",
    "     [0,1,1,1],\n",
    "     [1,0,0,0],\n",
    "     [0,1,0,0],\n",
    "     [1,0,0,0],\n",
    "     [1,0,1,2],\n",
    "     [1,1,1,1],\n",
    "     [1,1,1,0],\n",
    "     [1,0,0,0],\n",
    "     [0,2,1,1],\n",
    "     [0,1,1,0]]\n",
    "\n",
    "\n",
    "M = np.array(M)\n",
    "\n",
    "# compute SVD\n",
    "K,S,Dt = np.linalg.svd(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.41291701, -0.12294407,  0.05933248, -0.03660797, -0.26712177,\n",
       "        -0.64005397, -0.4112895 , -0.29358326, -0.26712177, -0.0331095 ,\n",
       "        -0.02646149],\n",
       "       [-0.3359611 ,  0.1962311 , -0.25246121,  0.11968319,  0.21609294,\n",
       "         0.20940104, -0.13578172, -0.22687177,  0.21609294, -0.60596737,\n",
       "        -0.44296471],\n",
       "       [-0.07695592, -0.31917516,  0.31179369, -0.15629115, -0.34608445,\n",
       "         0.63322438, -0.05581269, -0.33567917, -0.34608445, -0.11889882,\n",
       "         0.01040529],\n",
       "       [-0.11909604,  0.2663899 ,  0.20432237, -0.52093504, -0.02675801,\n",
       "        -0.03718789, -0.05980868,  0.39531815, -0.02675801, -0.51079828,\n",
       "         0.42207616],\n",
       "       [-0.07695592, -0.31917516,  0.31179369, -0.15629115,  0.84973674,\n",
       "        -0.0397257 , -0.08480244, -0.11551346, -0.15026326,  0.0510951 ,\n",
       "         0.03474981],\n",
       "       [-0.39922386, -0.49767812, -0.57172873,  0.04465203,  0.02360429,\n",
       "         0.19154746, -0.12789716,  0.33181808,  0.02360429,  0.06933868,\n",
       "         0.30821378],\n",
       "       [-0.41291701, -0.12294407,  0.05933248, -0.03660797, -0.04654205,\n",
       "        -0.18117734,  0.87095404, -0.07968105, -0.04654205, -0.07973746,\n",
       "        -0.03313899],\n",
       "       [-0.30751414, -0.01459992,  0.48607132,  0.40306708, -0.06332949,\n",
       "         0.07591088, -0.1063498 ,  0.60815231, -0.06332949,  0.06021689,\n",
       "        -0.3285182 ],\n",
       "       [-0.07695592, -0.31917516,  0.31179369, -0.15629115, -0.15026326,\n",
       "        -0.0397257 , -0.08480244, -0.11551346,  0.84973674,  0.0510951 ,\n",
       "         0.03474981],\n",
       "       [-0.45505713,  0.462621  , -0.04813884, -0.40125186,  0.0503623 ,\n",
       "         0.22873535, -0.06808849, -0.06350007,  0.0503623 ,  0.58013696,\n",
       "        -0.11386237],\n",
       "       [-0.23055822,  0.30457524,  0.17427762,  0.55935823,  0.08693378,\n",
       "         0.11563658, -0.02154736, -0.27633423,  0.08693378,  0.00912179,\n",
       "         0.63673199]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.78695453, 2.31848919, 1.762346  , 0.77705263])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.36838448, -0.57010731, -0.53356439, -0.50455879],\n",
       "       [-0.74000417,  0.61762211,  0.0885323 , -0.25119473],\n",
       "       [ 0.54948837,  0.36008671, -0.05294924, -0.75206148],\n",
       "       [-0.12144645, -0.40479395,  0.83944473, -0.34165065]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Question 1.b\n",
    "\n",
    "Are the values of $S$ sorted? Perform latent semantic indexing by selecting the first two largest singular values of the matrix $S$.\n",
    "\n",
    "<b>Hint:</b> See the lecture slides on latent semantic indexing for more details. A sub-matrix of a numpy matrix can be computed using indexing operations (see https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_sel = K[:,[0,1]]\n",
    "S_sel = np.diag(S[:2])\n",
    "Dt_sel = Dt[[0,1],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.41291701, -0.12294407],\n",
       "       [-0.3359611 ,  0.1962311 ],\n",
       "       [-0.07695592, -0.31917516],\n",
       "       [-0.11909604,  0.2663899 ],\n",
       "       [-0.07695592, -0.31917516],\n",
       "       [-0.39922386, -0.49767812],\n",
       "       [-0.41291701, -0.12294407],\n",
       "       [-0.30751414, -0.01459992],\n",
       "       [-0.07695592, -0.31917516],\n",
       "       [-0.45505713,  0.462621  ],\n",
       "       [-0.23055822,  0.30457524]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.78695453, 0.        ],\n",
       "       [0.        , 2.31848919]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.36838448, -0.57010731, -0.53356439, -0.50455879],\n",
       "       [-0.74000417,  0.61762211,  0.0885323 , -0.25119473]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dt_sel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 1.c\n",
    "\n",
    "Given the query $q$:\n",
    "\n",
    "$\n",
    "q =\n",
    "  \\begin{bmatrix}\n",
    "\t0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\\n",
    "  \\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "Map query $q$ into the new document space $D$. The new query is referred to as $q^*$. \n",
    "\n",
    "<b>Hint:</b> Use the formulation for mapping queries provided in the lecture slides. You can also use np.linalg.inv function for computing the inverse of a matrix.\n",
    "\n",
    "###  Question 1.d\n",
    "\n",
    "Arrange the documents based on the cosine similarity measure between $q^*$ and the new documents in the space $D$.\n",
    "\n",
    "<b>Hint:</b> Use the cosine similarity function from the previous exercise on vector space retrieval.\n",
    "\n",
    "###  Question 1.e\n",
    "\n",
    "Does the order of documents change if document $d_3$ is dropped? If yes, why? \n",
    "If no, how should $d_3$ be modified to change the document ordering?\n",
    "\n",
    "\n",
    "### Question 1.f [Optional]\n",
    "\n",
    "Run latent semantic indexing for the document collection presented in the previous exercise (presented here as well):\n",
    "\n",
    "  DocID | Document Text\n",
    "  ------|------------------\n",
    "  1     | How to Bake Breads Without Baking Recipes\n",
    "  2     | Smith Pies: Best Pies in London\n",
    "  3     | Numerical Recipes: The Art of Scientific Computing\n",
    "  4     | Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes\n",
    "  5     | Pastry: A Book of Best French Pastry Recipes\n",
    "\n",
    "Now, for the query $Q=$''<i>baking</i>'', find the top ranked documents according to LSI (use three singular values). \n",
    "\n",
    "<b>Hint:</b> Use the code for computing document_vectors from the last exercise. However note that document_vectors represent document-term matrix whereas LSI uses term-document matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.22662409,  0.11624731]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = [\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "[1],\n",
    "[0],\n",
    "[0],\n",
    "[0],\n",
    "[1],\n",
    "[1]]\n",
    "q = np.array(q)\n",
    "q_star = q.T@K_sel@np.linalg.inv(S_sel)\n",
    "q_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The doc 3 has similarity 0.9524776244205612\n",
      "The doc 2 has similarity 0.9388827727147445\n",
      "The doc 4 has similarity 0.5931086268074788\n",
      "The doc 1 has similarity -0.012057913278690343\n"
     ]
    }
   ],
   "source": [
    "# compute cosine similarity.\n",
    "    \n",
    "import math\n",
    "\n",
    "# Function for computing cosine similarity.\n",
    "def cosine_similarity(v1, v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy*1.0/math.sqrt(sumxx*sumyy)\n",
    "\n",
    "docs_sim = {}\n",
    "for i in range(M.shape[1]):\n",
    "    simlar = cosine_similarity(Dt_sel[:,i], q_star[0])\n",
    "    docs_sim[i] = simlar\n",
    "\n",
    "sorted_docs = dict(sorted(docs_sim.items(), key=lambda item:-item[1]))\n",
    "\n",
    "for doc_id, similarity in sorted_docs.items():\n",
    "    print(\"The doc {0} has similarity {1}\".format(doc_id+1, similarity))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ravin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['art',\n",
       " 'bake',\n",
       " 'best',\n",
       " 'book',\n",
       " 'bread',\n",
       " 'cake',\n",
       " 'comput',\n",
       " 'french',\n",
       " 'london',\n",
       " 'numer',\n",
       " 'pastri',\n",
       " 'pie',\n",
       " 'quantiti',\n",
       " 'recip',\n",
       " 'scientif',\n",
       " 'smith',\n",
       " 'without']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"bread.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = set([item for sublist in documents for item in sublist])\n",
    "vocabulary = [word for word in vocabulary if word not in stopwords.words('english')]\n",
    "vocabulary.sort()\n",
    "\n",
    "# compute IDF, storing idf values in a dictionary\n",
    "def idf_values(vocabulary, documents):\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        idf[term] = math.log(num_documents/sum(term in document for document in documents), math.e)\n",
    "    return idf\n",
    "\n",
    "# Function to generate the vector for a document (with normalisation)\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] = idf[term] * counts[term]/max_count\n",
    "    return vector\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]\n",
    "\n",
    "vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.matrix.transpose(np.array(document_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 1.60943791, 0.        , 0.        ],\n",
       "       [0.91629073, 0.        , 0.        , 0.91629073, 0.        ],\n",
       "       [0.        , 0.45814537, 0.        , 0.        , 0.45814537],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.80471896],\n",
       "       [0.45814537, 0.        , 0.        , 0.91629073, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 1.60943791, 0.        ],\n",
       "       [0.        , 0.        , 1.60943791, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.80471896],\n",
       "       [0.        , 0.80471896, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.60943791, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.91629073, 0.91629073],\n",
       "       [0.        , 0.91629073, 0.        , 0.91629073, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 1.60943791, 0.        ],\n",
       "       [0.11157178, 0.        , 0.22314355, 0.22314355, 0.11157178],\n",
       "       [0.        , 0.        , 1.60943791, 0.        , 0.        ],\n",
       "       [0.        , 0.80471896, 0.        , 0.        , 0.        ],\n",
       "       [0.80471896, 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI\n",
    "\n",
    "K,S,Dt = np.linalg.svd(M)\n",
    "\n",
    "K_sel = K[:,:3]\n",
    "S_sel = np.diag(S[:3])\n",
    "Dt_sel = Dt[:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.22696189, 0.        , 0.        ],\n",
       "       [0.        , 3.00604272, 0.        ],\n",
       "       [0.        , 0.        , 1.54530928]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00416487,  0.11537136, -0.14603541])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = np.array([0]*len(vocabulary))\n",
    "\n",
    "#Set the term corresponding to baking = 1\n",
    "q[1] = 1\n",
    "\n",
    "q_star = q.T@K_sel@np.linalg.inv(S_sel)\n",
    "q_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The doc 1 has similarity 0.9980518678772611\n",
      "The doc 4 has similarity 0.7231078789682566\n",
      "The doc 3 has similarity -0.0023288750529669527\n",
      "The doc 5 has similarity -0.6551062911362043\n",
      "The doc 2 has similarity -0.6577609566355738\n"
     ]
    }
   ],
   "source": [
    "docs_sim = {}\n",
    "for i in range(M.shape[1]):\n",
    "    simlar = cosine_similarity(Dt_sel[:,i], q_star)\n",
    "    docs_sim[i] = simlar\n",
    "\n",
    "sorted_docs = dict(sorted(docs_sim.items(), key=lambda item:-item[1]))\n",
    "\n",
    "for doc_id, similarity in sorted_docs.items():\n",
    "    print(\"The doc {0} has similarity {1}\".format(doc_id+1, similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
