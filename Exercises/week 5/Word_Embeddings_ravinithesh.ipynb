{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link based ranking\n",
    "\n",
    "**Preliminaries:** If you want to normalize a vector to L1-norm or L2-norm, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "\n",
    "pr = np.array([1,2,3])\n",
    "print(\"L1-norm of {0} is {1}\".format(pr, pr / np.linalg.norm(pr,1)))\n",
    "print(\"L2-norm of {0} is {1}\".format(pr, pr / np.linalg.norm(pr,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Rank (Eigen-vector method)\n",
    "Consider a tiny Web with three pages A, B and C with no inlinks,\n",
    "and with initial PageRank = 1. Initially, none of the pages link to\n",
    "any other pages and none link to them. \n",
    "Answer the following questions, and calculate the PageRank for\n",
    "each question.\n",
    "\n",
    "1. Link page A to page B.\n",
    "2. Link all pages to each other.\n",
    "3. Link page A to both B and C, and link pages B and C to A.\n",
    "4. Use the previous links and add a link from page C to page B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints:**\n",
    "We are using the theoretical PageRank computation (without source of rank). See slide \"Transition Matrix for Random Walker\" in the lecture note. Columns of link matrix are from-vertex, rows of link matrix are to-vertex. We take the eigenvector with the largest eigenvalue.\n",
    "We only care about final ranking of the probability vector. You can choose the normalization (or not) of your choice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your code here\n",
    "def pagerank_eigen(L):\n",
    "#   Construct transition probability matrix from L\n",
    "    R = ...\n",
    "#     Compute eigen-vectors and eigen-values of R\n",
    "    eigenvalues, eigenvectors = ...\n",
    "#     Take the eigen-vector with maximum eigven-value\n",
    "    p = ...\n",
    "    return (R,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with the question, e.g.\n",
    "L = np.matrix([\n",
    "    [0,1,1], \n",
    "    [1,0,1], \n",
    "    [1,1,0]\n",
    "])\n",
    "R,p = pagerank_eigen(L)\n",
    "print(\"L={0}\\nR={1}\\np={2}\".format(L,R,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Rank (Iterative method)\n",
    "\n",
    "The eigen-vector method has some numerical issues (when computing eigen-vector) and not scalable with large datasets.\n",
    "\n",
    "We will apply the iterative method in the slide \"Practical Computation of PageRank\" of the lecture.\n",
    "\n",
    "Dataset for practice: https://snap.stanford.edu/data/ca-GrQc.html. It is available within the same folder of this github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank_iterative(L):\n",
    "    R = ...\n",
    "    N = L.shape(0)\n",
    "    e = np.ones(shape=(N,1))\n",
    "    q = 0.9\n",
    "\n",
    "    p = e\n",
    "    delta = 1\n",
    "    epsilon = 0.001\n",
    "    i = 0\n",
    "    while delta > epsilon:\n",
    "        p_prev = p\n",
    "        p = ...\n",
    "        p = p + ...\n",
    "        delta = ...\n",
    "        i += 1\n",
    "\n",
    "    print(\"Converged after {0} iterations. Ranking vector: p={1}\".format(i, p[:,0]))\n",
    "    return R,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct link matrix from file\n",
    "f = open(\"ca-GrQc.txt\")\n",
    "L = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PageRank\n",
    "R, p = pagerank_iterative(L)\n",
    "print(\"Ranking vector: p={0}\".format(p[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Methodology (Hard)\n",
    "\n",
    "1. Give a directed graph, as small as possible, satisfying all the properties mentioned below:\n",
    "\n",
    "    1. There exists a path from node i to node j for all nodes i,j in the directed\n",
    "graph. Recall, with this property the jump to an arbitrary node in PageRank\n",
    "is not required, so that you can set q = 1 (refer lecture slides).\n",
    "\n",
    "    2. HITS authority ranking and PageRank ranking of the graph nodes are different.\n",
    "\n",
    "2. Give intuition/methodology on how you constructed such a directed graph with\n",
    "the properties described in (a).\n",
    "\n",
    "3. Are there specific graph structures with arbitrarily large instances where PageRank\n",
    "ranking and HITS authority ranking are the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 - Hub and Authority\n",
    "\n",
    "### a)\n",
    "\n",
    "Let the adjacency matrix for a graph of four vertices ($n_1$ to $n_4$) be\n",
    "as follows:\n",
    "\n",
    "$\n",
    "A =\n",
    "  \\begin{bmatrix}\n",
    "\t0 & 1 & 1 & 1  \\\\\n",
    "\t0 & 0 & 1 & 1 \\\\\n",
    "\t1 & 0 & 0 & 1 \\\\\n",
    "\t0 & 0 & 0 & 1 \\\\\n",
    "  \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Calculate the authority and hub scores for this graph using the\n",
    "HITS algorithm with k = 6, and identify the best authority and\n",
    "hub nodes.\n",
    "\n",
    "### b)\n",
    "Apply the HITS algorithm to the dataset: https://snap.stanford.edu/data/ca-GrQc.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** We follow the slide \"HITS algorithm\" in the lecture. **Denote $x$ as authority vector and $y$ as hub vector**. You can use matrix multiplication for the update steps in the slide \"Convergence of HITS\". Note that rows of adjacency matrix is from-vertex and columns of adjacency matrix is to-vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can implement your code following this template.\n",
    "def hits_iterative(A, k=10):\n",
    "    N = A.shape[0]\n",
    "\n",
    "    x0, y0 = 1 / (N*N) * np.ones(N), 1 / (N*N) * np.ones(N) \n",
    "\n",
    "    xprev, yprev = x0, y0\n",
    "    \n",
    "    # For advanced exercise: define a convergence condition instead of k iterations\n",
    "    for l in range(0,k):\n",
    "        y = ...\n",
    "        x = ...\n",
    "        xprev = ...\n",
    "        yprev = ...\n",
    "        \n",
    "    return xprev, yprev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Word Embeddings\n",
    "\n",
    "In this exercise, we would train word embeddings using a state-of-the-art embeddings library fastText. The first step of the exercise is to install the fasttext library. Proceed with the following steps:\n",
    "\n",
    "## FastText installation\n",
    "\n",
    "> pip install fasttext\n",
    "\n",
    "If you are having problems, try this command:\n",
    "> sudo apt-get install g++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_unsupervised('epfldocs.txt', model = 'cbow')\n",
    "vocabulary = model.words\n",
    "word_embeddings = np.array([model[word] for word in vocabulary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Pretrained Embeddings\n",
    "If you are unable to install fasttext, you can use the preembeddings we provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import codecs\n",
    "import numpy as np\n",
    "\n",
    "# Edit this, or just move model_epfldocs.vec to the directory where this notebook is situated\n",
    "directory_path = './'\n",
    "\n",
    "def load_embeddings(file_name):\n",
    "    with codecs.open(file_name, 'r', 'utf-8') as f_in:\n",
    "        lines = f_in.readlines()\n",
    "        lines = lines[1:]\n",
    "        vocabulary, wv = zip(*[line.strip().split(' ', 1) for line in lines])\n",
    "    wv = np.loadtxt(wv)\n",
    "    return wv, vocabulary\n",
    "\n",
    "\n",
    "# Replace the path based on your own machine.\n",
    "word_embeddings, vocabulary = load_embeddings(directory_path + 'model_epfldocs.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Visualize Embeddings\n",
    "\n",
    "In the third phase of this exercise, we will visualize the generated embeddings using t-SNE (T-Distributed Stochastic Neighbouring Entities). t-SNE is a dimensionality reduction algorithm which is well suited for such visualization tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=1000, init = 'pca') \n",
    "vis_data = tsne.fit_transform(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_data_x = vis_data[:,0]\n",
    "vis_data_y = vis_data[:,1]\n",
    "\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "plt.figure(figsize=(40, 40)) \n",
    "plt.scatter(vis_data_x, vis_data_y)\n",
    "\n",
    "for label, x, y in zip(vocabulary, vis_data_x, vis_data_y):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Observe the plot of word embeddings. Do you observe any patterns?\n",
    "\n",
    "\n",
    "\n",
    "## Question\n",
    "\n",
    "Write a python function to find the most similar terms for a given term. The similarity between two terms is defined as the cosine similarity between their corresponding word embeddings.\n",
    "\n",
    "Find the top 5 terms that are most similar to 'la', 'EPFL', '#robot', 'this', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_most_similar(input_term, word_embeddings, vocabulary, num_terms=5):\n",
    "    # Fill in your code\n",
    "\n",
    "find_most_similar('EPFL', word_embeddings, vocabulary, num_terms=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question [Optional]\n",
    "​\n",
    "Observe the word embeddings that are visualized in this link http://www.anthonygarvan.com/wordgalaxy/ . Can you make some interesting observations? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Search Engine Using Word Embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we would put our word embeddings to test by using them for information retrieval. \n",
    "The idea is that, the documents that have the most similar embedding vectors to the one belongs to query should rank higher.\n",
    "The documents may not necessarily include the keywords in the query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of libraries and documents\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"epfldocs.txt\", ) as f:\n",
    "    content = f.readlines()\n",
    "        \n",
    "original_documents = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "Since both the documents and the query is of variable size, we should aggregate the vectors of the words in the query by some strategy. This could be taking the minimum vector, maximum vector or the mean. Fill in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of vectors for easier search\n",
    "vector_dict = dict(zip(vocabulary, word_embeddings))\n",
    "\n",
    "def aggregate_vector_list(vlist, aggfunc):\n",
    "    if aggfunc == 'max':\n",
    "        return # TODO\n",
    "    elif aggfunc == 'min':\n",
    "        return # TODO \n",
    "    elif aggfunc == 'mean':\n",
    "        return # TODO\n",
    "    else:\n",
    "        return np.zeros(np.array(vlist).shape[1])\n",
    "\n",
    "possible_aggfuncs = [\"max\", \"min\", \"mean\"]\n",
    "\n",
    "aggregated_doc_vectors = {}\n",
    "\n",
    "# Aggregate vectors of documents beforehand\n",
    "# TODO\n",
    "for aggfunc in possible_aggfuncs:\n",
    "    aggregated_doc_vectors[aggfunc] = np.zeros((len(original_documents), word_embeddings.shape[1]))\n",
    "    for index, doc in enumerate(original_documents):\n",
    "        vlist = # TODO\n",
    "        aggregated_doc_vectors[aggfunc][index] = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "Aggregate the query and find the most similar documents using cosine distance between the query's vector and document's aggregated vector. Are they seem to relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query = \"EPFL\"\n",
    "\n",
    "def aggregate_query(query, aggfunc):\n",
    "    # TODO\n",
    "    # Raise an error message for the case when there is no words in the query that is included in the vocabulary\n",
    "    # This should return a vector of shape (1, word_embeddings.shape[1])\n",
    "    \n",
    "def get_most_similar_documents(query_vector, aggfunc, k = 5):\n",
    "    # Calculate the similarity with each document vector. \n",
    "    # Hint: Cosine similarity function takes a matrix as input so you do not need to loop through each document vector.\n",
    "    sim = # TODO\n",
    "    \n",
    "    # Rank the document vectors according to their cosine similarity with the query vector and return topk indexes\n",
    "    indexes = # TODO\n",
    "\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def search_vec_embeddings(query, topk = 10, aggfunc = 'mean'):\n",
    "    query_vector = aggregate_query(query, aggfunc)\n",
    "    indexes = get_most_similar_documents(query_vector, aggfunc)\n",
    "    # Print the top k documents\n",
    "    indexes = indexes[0:topk]\n",
    "    for index in indexes:\n",
    "        print(original_documents[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_vec_embeddings('EPFL', aggfunc = 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "Compare the results with the vector space retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTOR SPACE RETRIEVAL (From Exercise 1)\n",
    "# Retrieval oracle \n",
    "from operator import itemgetter\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english')\n",
    "features = tf.fit_transform(original_documents)\n",
    "npm_tfidf = features.todense()\n",
    "\n",
    "# Return all document ids that that have cosine similarity with the query larger than a threshold\n",
    "def search_vec_sklearn(query, topk = 10, features = features, threshold=0.1):\n",
    "    new_features = tf.transform([query])\n",
    "    cosine_similarities = cosine_similarity(new_features, features).flatten()\n",
    "    related_docs_indices, cos_sim_sorted = zip(*sorted(enumerate(cosine_similarities), key=itemgetter(1), \n",
    "                                                       reverse=True))\n",
    "    doc_ids = []\n",
    "    for i, cos_sim in enumerate(cos_sim_sorted):\n",
    "        if cos_sim < threshold or i >= topk:\n",
    "            break\n",
    "        doc_ids.append(related_docs_indices[i])\n",
    "    \n",
    "    for index in doc_ids:\n",
    "        print(original_documents[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_vec_embeddings('EPFL', aggfunc = 'mean')\n",
    "print(\"---------------------------------\")\n",
    "search_vec_sklearn(\"EPFL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "You will realize that not all the words in your queries are in the vocabulary, so your queries fail to retrieve any documents. Think of possible solutions to overcome this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
